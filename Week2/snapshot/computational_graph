digraph {
	graph [size="12.45,12.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140406623238032 [label="
 (1, 11)" fillcolor=darkolivegreen1]
	140406615777168 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (1, 12288)
mat1_sym_strides:     (12288, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :    (12288, 11)
mat2_sym_strides:     (1, 12288)"]
	140406615777456 -> 140406615777168
	140406615868816 [label="output_layer.bias
 (11)" fillcolor=lightblue]
	140406615868816 -> 140406615777456
	140406615777456 [label=AccumulateGrad]
	140406615777360 -> 140406615777168
	140406615777360 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140406615777312 -> 140406615777360
	140406615777312 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (1, 18432)
mat1_sym_strides:     (18432, 1)
mat2            : [saved tensor]
mat2_sym_sizes  : (18432, 12288)
mat2_sym_strides:     (1, 18432)"]
	140406615777600 -> 140406615777312
	140406615868576 [label="layer5.bias
 (12288)" fillcolor=lightblue]
	140406615868576 -> 140406615777600
	140406615777600 [label=AccumulateGrad]
	140406615777552 -> 140406615777312
	140406615777552 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140406615777696 -> 140406615777552
	140406615777696 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (1, 24576)
mat1_sym_strides:     (24576, 1)
mat2            : [saved tensor]
mat2_sym_sizes  : (24576, 18432)
mat2_sym_strides:     (1, 24576)"]
	140406615777888 -> 140406615777696
	140406615868656 [label="layer4.bias
 (18432)" fillcolor=lightblue]
	140406615868656 -> 140406615777888
	140406615777888 [label=AccumulateGrad]
	140406615777840 -> 140406615777696
	140406615777840 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140406615777984 -> 140406615777840
	140406615777984 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (1, 18432)
mat1_sym_strides:     (18432, 1)
mat2            : [saved tensor]
mat2_sym_sizes  : (18432, 24576)
mat2_sym_strides:     (1, 18432)"]
	140406615778176 -> 140406615777984
	140406615868496 [label="layer3.bias
 (24576)" fillcolor=lightblue]
	140406615868496 -> 140406615778176
	140406615778176 [label=AccumulateGrad]
	140406615778128 -> 140406615777984
	140406615778128 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140406615778272 -> 140406615778128
	140406615778272 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (1, 12288)
mat1_sym_strides:     (12288, 1)
mat2            : [saved tensor]
mat2_sym_sizes  : (12288, 18432)
mat2_sym_strides:     (1, 12288)"]
	140406615778464 -> 140406615778272
	140406615868176 [label="layer2.bias
 (18432)" fillcolor=lightblue]
	140406615868176 -> 140406615778464
	140406615778464 [label=AccumulateGrad]
	140406615778416 -> 140406615778272
	140406615778416 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140406615778560 -> 140406615778416
	140406615778560 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (1, 12288)
mat1_sym_strides:             ()
mat2            :           None
mat2_sym_sizes  : (12288, 12288)
mat2_sym_strides:     (1, 12288)"]
	140406615778752 -> 140406615778560
	140406623233632 [label="layer1.bias
 (12288)" fillcolor=lightblue]
	140406623233632 -> 140406615778752
	140406615778752 [label=AccumulateGrad]
	140406615778704 -> 140406615778560
	140406615778704 [label=TBackward0]
	140406615778800 -> 140406615778704
	140406623605664 [label="layer1.weight
 (12288, 12288)" fillcolor=lightblue]
	140406623605664 -> 140406615778800
	140406615778800 [label=AccumulateGrad]
	140406615778368 -> 140406615778272
	140406615778368 [label=TBackward0]
	140406615778848 -> 140406615778368
	140406626821808 [label="layer2.weight
 (18432, 12288)" fillcolor=lightblue]
	140406626821808 -> 140406615778848
	140406615778848 [label=AccumulateGrad]
	140406615778080 -> 140406615777984
	140406615778080 [label=TBackward0]
	140406615778656 -> 140406615778080
	140406615868336 [label="layer3.weight
 (24576, 18432)" fillcolor=lightblue]
	140406615868336 -> 140406615778656
	140406615778656 [label=AccumulateGrad]
	140406615777792 -> 140406615777696
	140406615777792 [label=TBackward0]
	140406615778512 -> 140406615777792
	140406615868416 [label="layer4.weight
 (18432, 24576)" fillcolor=lightblue]
	140406615868416 -> 140406615778512
	140406615778512 [label=AccumulateGrad]
	140406615777264 -> 140406615777312
	140406615777264 [label=TBackward0]
	140406615778224 -> 140406615777264
	140406615868256 [label="layer5.weight
 (12288, 18432)" fillcolor=lightblue]
	140406615868256 -> 140406615778224
	140406615778224 [label=AccumulateGrad]
	140406615777408 -> 140406615777168
	140406615777408 [label=TBackward0]
	140406615777936 -> 140406615777408
	140406615868736 [label="output_layer.weight
 (11, 12288)" fillcolor=lightblue]
	140406615868736 -> 140406615777936
	140406615777936 [label=AccumulateGrad]
	140406615777168 -> 140406623238032
}
