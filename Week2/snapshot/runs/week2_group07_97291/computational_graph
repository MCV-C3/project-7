digraph {
	graph [size="12.45,12.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140121891469120 [label="
 (1, 11)" fillcolor=darkolivegreen1]
	140121891415728 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 600)
mat1_sym_strides:       (600, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (600, 11)
mat2_sym_strides:       (1, 600)"]
	140121891416016 -> 140121891415728
	140121891469040 [label="output_layer.bias
 (11)" fillcolor=lightblue]
	140121891469040 -> 140121891416016
	140121891416016 [label=AccumulateGrad]
	140121891415920 -> 140121891415728
	140121891415920 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140121891415872 -> 140121891415920
	140121891415872 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 300)
mat1_sym_strides:       (300, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (300, 600)
mat2_sym_strides:       (1, 300)"]
	140121891416160 -> 140121891415872
	140121891468880 [label="layer5.bias
 (600)" fillcolor=lightblue]
	140121891468880 -> 140121891416160
	140121891416160 [label=AccumulateGrad]
	140121891416112 -> 140121891415872
	140121891416112 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140121891416256 -> 140121891416112
	140121891416256 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 150)
mat1_sym_strides:       (150, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (150, 300)
mat2_sym_strides:       (1, 150)"]
	140121891416448 -> 140121891416256
	140121891468720 [label="layer4.bias
 (300)" fillcolor=lightblue]
	140121891468720 -> 140121891416448
	140121891416448 [label=AccumulateGrad]
	140121891416400 -> 140121891416256
	140121891416400 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140121891416544 -> 140121891416400
	140121891416544 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 300)
mat1_sym_strides:       (300, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (300, 150)
mat2_sym_strides:       (1, 300)"]
	140121891416736 -> 140121891416544
	140121891468240 [label="layer3.bias
 (150)" fillcolor=lightblue]
	140121891468240 -> 140121891416736
	140121891416736 [label=AccumulateGrad]
	140121891416688 -> 140121891416544
	140121891416688 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140121891416832 -> 140121891416688
	140121891416832 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 600)
mat1_sym_strides:       (600, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (600, 300)
mat2_sym_strides:       (1, 600)"]
	140121891417024 -> 140121891416832
	140121891468480 [label="layer2.bias
 (300)" fillcolor=lightblue]
	140121891468480 -> 140121891417024
	140121891417024 [label=AccumulateGrad]
	140121891416976 -> 140121891416832
	140121891416976 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140121891417120 -> 140121891416976
	140121891417120 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (1, 150528)
mat1_sym_strides:             ()
mat2            :           None
mat2_sym_sizes  :  (150528, 600)
mat2_sym_strides:    (1, 150528)"]
	140121891417312 -> 140121891417120
	140121891468400 [label="layer1.bias
 (600)" fillcolor=lightblue]
	140121891468400 -> 140121891417312
	140121891417312 [label=AccumulateGrad]
	140121891417264 -> 140121891417120
	140121891417264 [label=TBackward0]
	140121891417360 -> 140121891417264
	140121891468320 [label="layer1.weight
 (600, 150528)" fillcolor=lightblue]
	140121891468320 -> 140121891417360
	140121891417360 [label=AccumulateGrad]
	140121891416928 -> 140121891416832
	140121891416928 [label=TBackward0]
	140121891417408 -> 140121891416928
	140121891467280 [label="layer2.weight
 (300, 600)" fillcolor=lightblue]
	140121891467280 -> 140121891417408
	140121891417408 [label=AccumulateGrad]
	140121891416640 -> 140121891416544
	140121891416640 [label=TBackward0]
	140121891417216 -> 140121891416640
	140121891468640 [label="layer3.weight
 (150, 300)" fillcolor=lightblue]
	140121891468640 -> 140121891417216
	140121891417216 [label=AccumulateGrad]
	140121891416352 -> 140121891416256
	140121891416352 [label=TBackward0]
	140121891417072 -> 140121891416352
	140121891468560 [label="layer4.weight
 (300, 150)" fillcolor=lightblue]
	140121891468560 -> 140121891417072
	140121891417072 [label=AccumulateGrad]
	140121891415824 -> 140121891415872
	140121891415824 [label=TBackward0]
	140121891416784 -> 140121891415824
	140121891468800 [label="layer5.weight
 (600, 300)" fillcolor=lightblue]
	140121891468800 -> 140121891416784
	140121891416784 [label=AccumulateGrad]
	140121891415968 -> 140121891415728
	140121891415968 [label=TBackward0]
	140121891416496 -> 140121891415968
	140121891468960 [label="output_layer.weight
 (11, 600)" fillcolor=lightblue]
	140121891468960 -> 140121891416496
	140121891416496 [label=AccumulateGrad]
	140121891415728 -> 140121891469120
}
