digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140427752550976 [label="
 (1, 11)" fillcolor=darkolivegreen1]
	140427752497904 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 300)
mat1_sym_strides:       (300, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (300, 11)
mat2_sym_strides:       (1, 300)"]
	140427752498192 -> 140427752497904
	140427752550656 [label="output_layer.bias
 (11)" fillcolor=lightblue]
	140427752550656 -> 140427752498192
	140427752498192 [label=AccumulateGrad]
	140427752498096 -> 140427752497904
	140427752498096 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140427752498048 -> 140427752498096
	140427752498048 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 300)
mat1_sym_strides:       (300, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (300, 300)
mat2_sym_strides:       (1, 300)"]
	140427752498336 -> 140427752498048
	140427759005552 [label="layer2.bias
 (300)" fillcolor=lightblue]
	140427759005552 -> 140427752498336
	140427752498336 [label=AccumulateGrad]
	140427752498288 -> 140427752498048
	140427752498288 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140427752498432 -> 140427752498288
	140427752498432 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (1, 150528)
mat1_sym_strides:             ()
mat2            :           None
mat2_sym_sizes  :  (150528, 300)
mat2_sym_strides:    (1, 150528)"]
	140427752498624 -> 140427752498432
	140427755157312 [label="layer1.bias
 (300)" fillcolor=lightblue]
	140427755157312 -> 140427752498624
	140427752498624 [label=AccumulateGrad]
	140427752498576 -> 140427752498432
	140427752498576 [label=TBackward0]
	140427752498672 -> 140427752498576
	140427759904592 [label="layer1.weight
 (300, 150528)" fillcolor=lightblue]
	140427759904592 -> 140427752498672
	140427752498672 [label=AccumulateGrad]
	140427752498000 -> 140427752498048
	140427752498000 [label=TBackward0]
	140427752498720 -> 140427752498000
	140427761655120 [label="layer2.weight
 (300, 300)" fillcolor=lightblue]
	140427761655120 -> 140427752498720
	140427752498720 [label=AccumulateGrad]
	140427752498144 -> 140427752497904
	140427752498144 [label=TBackward0]
	140427752498528 -> 140427752498144
	140427752550576 [label="output_layer.weight
 (11, 300)" fillcolor=lightblue]
	140427752550576 -> 140427752498528
	140427752498528 [label=AccumulateGrad]
	140427752497904 -> 140427752550976
}
