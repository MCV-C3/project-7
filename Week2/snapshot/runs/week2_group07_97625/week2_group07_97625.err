/ghome/group07/miniconda3/envs/c3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
TRAINING THE MODEL:   0%|          | 0/30 [00:00<?, ?it/s]/ghome/group07/miniconda3/envs/c3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
TRAINING THE MODEL:   0%|          | 0/30 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "/export/home/group07/week2/main.py", line 172, in <module>
    train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)
  File "/export/home/group07/week2/main.py", line 33, in train
    optimizer.step()
  File "/ghome/group07/miniconda3/envs/c3/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/ghome/group07/miniconda3/envs/c3/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/ghome/group07/miniconda3/envs/c3/lib/python3.10/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/ghome/group07/miniconda3/envs/c3/lib/python3.10/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/ghome/group07/miniconda3/envs/c3/lib/python3.10/site-packages/torch/optim/adam.py", line 565, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 864.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 285.69 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 23.09 GiB is allocated by PyTorch, and 4.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
