digraph {
	graph [size="12.45,12.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140317443336816 [label="
 (1, 11)" fillcolor=darkolivegreen1]
	140317437912944 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (1, 8192)
mat1_sym_strides:      (8192, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (8192, 11)
mat2_sym_strides:      (1, 8192)"]
	140317437913232 -> 140317437912944
	140317435953616 [label="output_layer.bias
 (11)" fillcolor=lightblue]
	140317435953616 -> 140317437913232
	140317437913232 [label=AccumulateGrad]
	140317437913136 -> 140317437912944
	140317437913136 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140317437913088 -> 140317437913136
	140317437913088 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (1, 12288)
mat1_sym_strides:     (12288, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :  (12288, 8192)
mat2_sym_strides:     (1, 12288)"]
	140317437913376 -> 140317437913088
	140317435953456 [label="layer5.bias
 (8192)" fillcolor=lightblue]
	140317435953456 -> 140317437913376
	140317437913376 [label=AccumulateGrad]
	140317437913328 -> 140317437913088
	140317437913328 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140317437913472 -> 140317437913328
	140317437913472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (1, 16384)
mat1_sym_strides:     (16384, 1)
mat2            : [saved tensor]
mat2_sym_sizes  : (16384, 12288)
mat2_sym_strides:     (1, 16384)"]
	140317437913664 -> 140317437913472
	140317435953296 [label="layer4.bias
 (12288)" fillcolor=lightblue]
	140317435953296 -> 140317437913664
	140317437913664 [label=AccumulateGrad]
	140317437913616 -> 140317437913472
	140317437913616 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140317437913760 -> 140317437913616
	140317437913760 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (1, 12288)
mat1_sym_strides:     (12288, 1)
mat2            : [saved tensor]
mat2_sym_sizes  : (12288, 16384)
mat2_sym_strides:     (1, 12288)"]
	140317437913952 -> 140317437913760
	140317437935296 [label="layer3.bias
 (16384)" fillcolor=lightblue]
	140317437935296 -> 140317437913952
	140317437913952 [label=AccumulateGrad]
	140317437913904 -> 140317437913760
	140317437913904 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140317437914048 -> 140317437913904
	140317437914048 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :      (1, 8192)
mat1_sym_strides:      (8192, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :  (8192, 12288)
mat2_sym_strides:      (1, 8192)"]
	140317437914240 -> 140317437914048
	140317437935456 [label="layer2.bias
 (12288)" fillcolor=lightblue]
	140317437935456 -> 140317437914240
	140317437914240 [label=AccumulateGrad]
	140317437914192 -> 140317437914048
	140317437914192 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140317437914336 -> 140317437914192
	140317437914336 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (1, 12288)
mat1_sym_strides:             ()
mat2            :           None
mat2_sym_sizes  :  (12288, 8192)
mat2_sym_strides:     (1, 12288)"]
	140317437914528 -> 140317437914336
	140317443499376 [label="layer1.bias
 (8192)" fillcolor=lightblue]
	140317443499376 -> 140317437914528
	140317437914528 [label=AccumulateGrad]
	140317437914480 -> 140317437914336
	140317437914480 [label=TBackward0]
	140317437914576 -> 140317437914480
	140317437935376 [label="layer1.weight
 (8192, 12288)" fillcolor=lightblue]
	140317437935376 -> 140317437914576
	140317437914576 [label=AccumulateGrad]
	140317437914144 -> 140317437914048
	140317437914144 [label=TBackward0]
	140317437914624 -> 140317437914144
	140317437934656 [label="layer2.weight
 (12288, 8192)" fillcolor=lightblue]
	140317437934656 -> 140317437914624
	140317437914624 [label=AccumulateGrad]
	140317437913856 -> 140317437913760
	140317437913856 [label=TBackward0]
	140317437914432 -> 140317437913856
	140317437935536 [label="layer3.weight
 (16384, 12288)" fillcolor=lightblue]
	140317437935536 -> 140317437914432
	140317437914432 [label=AccumulateGrad]
	140317437913568 -> 140317437913472
	140317437913568 [label=TBackward0]
	140317437914288 -> 140317437913568
	140317435953216 [label="layer4.weight
 (12288, 16384)" fillcolor=lightblue]
	140317435953216 -> 140317437914288
	140317437914288 [label=AccumulateGrad]
	140317437913040 -> 140317437913088
	140317437913040 [label=TBackward0]
	140317437914000 -> 140317437913040
	140317435953376 [label="layer5.weight
 (8192, 12288)" fillcolor=lightblue]
	140317435953376 -> 140317437914000
	140317437914000 [label=AccumulateGrad]
	140317437913184 -> 140317437912944
	140317437913184 [label=TBackward0]
	140317437913712 -> 140317437913184
	140317435953536 [label="output_layer.weight
 (11, 8192)" fillcolor=lightblue]
	140317435953536 -> 140317437913712
	140317437913712 [label=AccumulateGrad]
	140317437912944 -> 140317443336816
}
